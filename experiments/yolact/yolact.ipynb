{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import warnings\n",
    "import torch\n",
    "import numpy\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load resnet backbone\n",
    "backbone = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = norm_layer(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\" Adapted from torchvision.models.resnet \"\"\"\n",
    "\n",
    "    def __init__(self, layers, block=Bottleneck, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_base_layers = len(layers)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.channels = []\n",
    "        self.norm_layer = norm_layer\n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self._make_layer(block, 64, layers[0])\n",
    "        self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                                                 kernel_size=1, stride=stride, bias=False),\n",
    "                                       self.norm_layer(planes * block.expansion))\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample, self.norm_layer)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer))\n",
    "\n",
    "        layer = nn.Sequential(*layers)\n",
    "\n",
    "        self.channels.append(planes * block.expansion)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Returns a list of convouts for each layer. \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        outs = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            outs.append(x)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def init_backbone(self, path):\n",
    "        \"\"\" Initializes the backbone weights for training. \"\"\"\n",
    "        state_dict = torch.load(path)\n",
    "        self.load_state_dict(state_dict, strict=True)\n",
    "        print(f'\\nBackbone is initiated with {path}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def box_iou(box_a, box_b):\n",
    "    \"\"\"\n",
    "    Compute the IoU of two sets of boxes.\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    use_batch = True\n",
    "    if box_a.dim() == 2:\n",
    "        use_batch = False\n",
    "        box_a = box_a[None, ...]\n",
    "        box_b = box_b[None, ...]\n",
    "\n",
    "    (n, A), B = box_a.shape[:2], box_b.shape[1]\n",
    "    # add a dimension\n",
    "    box_a = box_a[:, :, None, :].expand(n, A, B, 4)\n",
    "    box_b = box_b[:, None, :, :].expand(n, A, B, 4)\n",
    "\n",
    "    max_xy = torch.min(box_a[..., 2:], box_b[..., 2:])\n",
    "    min_xy = torch.max(box_a[..., :2], box_b[..., :2])\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    inter_area = inter[..., 0] * inter[..., 1]\n",
    "\n",
    "    area_a = (box_a[..., 2] - box_a[..., 0]) * (box_a[..., 3] - box_a[..., 1])\n",
    "    area_b = (box_b[..., 2] - box_b[..., 0]) * (box_b[..., 3] - box_b[..., 1])\n",
    "\n",
    "    out = inter_area / (area_a + area_b - inter_area)\n",
    "    return out if use_batch else out.squeeze(0)\n",
    "\n",
    "\n",
    "def box_iou_numpy(box_a, box_b):\n",
    "    (n, A), B = box_a.shape[:2], box_b.shape[1]\n",
    "    # add a dimension\n",
    "    box_a = np.tile(box_a[:, :, None, :], (1, 1, B, 1))\n",
    "    box_b = np.tile(box_b[:, None, :, :], (1, A, 1, 1))\n",
    "\n",
    "    max_xy = np.minimum(box_a[..., 2:], box_b[..., 2:])\n",
    "    min_xy = np.maximum(box_a[..., :2], box_b[..., :2])\n",
    "    inter = np.clip((max_xy - min_xy), a_min=0, a_max=100000)\n",
    "    inter_area = inter[..., 0] * inter[..., 1]\n",
    "\n",
    "    area_a = (box_a[..., 2] - box_a[..., 0]) * (box_a[..., 3] - box_a[..., 1])\n",
    "    area_b = (box_b[..., 2] - box_b[..., 0]) * (box_b[..., 3] - box_b[..., 1])\n",
    "\n",
    "    return inter_area / (area_a + area_b - inter_area)\n",
    "\n",
    "\n",
    "def match(cfg, box_gt, anchors, class_gt):\n",
    "    # Convert prior boxes to the form of [xmin, ymin, xmax, ymax].\n",
    "    decoded_priors = torch.cat((anchors[:, :2] - anchors[:, 2:] / 2, anchors[:, :2] + anchors[:, 2:] / 2), 1)\n",
    "\n",
    "    overlaps = box_iou(box_gt, decoded_priors)  # (num_gts, num_achors)\n",
    "\n",
    "    _, gt_max_i = overlaps.max(1)  # (num_gts, ) the max IoU for each gt box\n",
    "    each_anchor_max, anchor_max_i = overlaps.max(0)  # (num_achors, ) the max IoU for each anchor\n",
    "\n",
    "    # For the max IoU anchor for each gt box, set its IoU to 2. This ensures that it won't be filtered\n",
    "    # in the threshold step even if the IoU is under the negative threshold. This is because that we want\n",
    "    # at least one anchor to match with each gt box or else we'd be wasting training data.\n",
    "    each_anchor_max.index_fill_(0, gt_max_i, 2)\n",
    "\n",
    "    # Set the index of the pair (anchor, gt) we set the overlap for above.\n",
    "    for j in range(gt_max_i.size(0)):\n",
    "        anchor_max_i[gt_max_i[j]] = j\n",
    "\n",
    "    anchor_max_gt = box_gt[anchor_max_i]  # (num_achors, 4)\n",
    "\n",
    "    conf = class_gt[anchor_max_i] + 1  # the class of the max IoU gt box for each anchor\n",
    "    conf[each_anchor_max < cfg.pos_iou_thre] = -1  # label as neutral\n",
    "    conf[each_anchor_max < cfg.neg_iou_thre] = 0  # label as background\n",
    "\n",
    "    offsets = encode(anchor_max_gt, anchors)\n",
    "\n",
    "    return offsets, conf, anchor_max_gt, anchor_max_i\n",
    "\n",
    "\n",
    "def make_anchors(cfg, conv_h, conv_w, scale):\n",
    "    prior_data = []\n",
    "    # Iteration order is important (it has to sync up with the convout)\n",
    "    for j, i in product(range(conv_h), range(conv_w)):\n",
    "        # + 0.5 because priors are in center\n",
    "        x = (i + 0.5) / conv_w\n",
    "        y = (j + 0.5) / conv_h\n",
    "\n",
    "        for ar in cfg.aspect_ratios:\n",
    "            ar = sqrt(ar)\n",
    "            w = scale * ar / cfg.img_size\n",
    "            h = scale / ar / cfg.img_size\n",
    "\n",
    "            prior_data += [x, y, w, h]\n",
    "\n",
    "    return prior_data\n",
    "\n",
    "\n",
    "def encode(matched, priors):\n",
    "    variances = [0.1, 0.2]\n",
    "\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]  # 10 * (Xg - Xa) / Wa\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])  # 10 * (Yg - Ya) / Ha\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]  # 5 * log(Wg / Wa)\n",
    "    g_wh = torch.log(g_wh) / variances[1]  # 5 * log(Hg / Ha)\n",
    "    # return target for smooth_l1_loss\n",
    "    offsets = torch.cat([g_cxcy, g_wh], 1)  # [num_priors, 4]\n",
    "\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def sanitize_coordinates(_x1, _x2, img_size, padding=0):\n",
    "    \"\"\"\n",
    "    Sanitizes the input coordinates so that x1 < x2, x1 != x2, x1 >= 0, and x2 <= image_size.\n",
    "    Also converts from relative to absolute coordinates and casts the results to long tensors.\n",
    "\n",
    "    Warning: this does things in-place behind the scenes so copy if necessary.\n",
    "    \"\"\"\n",
    "    _x1 = _x1 * img_size\n",
    "    _x2 = _x2 * img_size\n",
    "\n",
    "    x1 = torch.min(_x1, _x2)\n",
    "    x2 = torch.max(_x1, _x2)\n",
    "    x1 = torch.clamp(x1 - padding, min=0)\n",
    "    x2 = torch.clamp(x2 + padding, max=img_size)\n",
    "\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "def sanitize_coordinates_numpy(_x1, _x2, img_size, padding=0):\n",
    "    _x1 = _x1 * img_size\n",
    "    _x2 = _x2 * img_size\n",
    "\n",
    "    x1 = np.minimum(_x1, _x2)\n",
    "    x2 = np.maximum(_x1, _x2)\n",
    "    x1 = np.clip(x1 - padding, a_min=0, a_max=1000000)\n",
    "    x2 = np.clip(x2 + padding, a_min=0, a_max=img_size)\n",
    "\n",
    "    return x1, x2\n",
    "\n",
    "\n",
    "def crop(masks, boxes, padding=1):\n",
    "    \"\"\"\n",
    "    \"Crop\" predicted masks by zeroing out everything not in the predicted bbox.\n",
    "    Args:\n",
    "        - masks should be a size [h, w, n] tensor of masks\n",
    "        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n",
    "    \"\"\"\n",
    "    h, w, n = masks.size()\n",
    "    x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding)\n",
    "    y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding)\n",
    "\n",
    "    rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)\n",
    "    cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)\n",
    "\n",
    "    masks_left = rows >= x1.view(1, 1, -1)\n",
    "    masks_right = rows < x2.view(1, 1, -1)\n",
    "    masks_up = cols >= y1.view(1, 1, -1)\n",
    "    masks_down = cols < y2.view(1, 1, -1)\n",
    "\n",
    "    crop_mask = masks_left * masks_right * masks_up * masks_down\n",
    "\n",
    "    return masks * crop_mask.float()\n",
    "\n",
    "\n",
    "def crop_numpy(masks, boxes, padding=1):\n",
    "    h, w, n = masks.shape\n",
    "    x1, x2 = sanitize_coordinates_numpy(boxes[:, 0], boxes[:, 2], w, padding)\n",
    "    y1, y2 = sanitize_coordinates_numpy(boxes[:, 1], boxes[:, 3], h, padding)\n",
    "\n",
    "    rows = np.tile(np.arange(w)[None, :, None], (h, 1, n))\n",
    "    cols = np.tile(np.arange(h)[:, None, None], (1, w, n))\n",
    "\n",
    "    masks_left = rows >= (x1.reshape(1, 1, -1))\n",
    "    masks_right = rows < (x2.reshape(1, 1, -1))\n",
    "    masks_up = cols >= (y1.reshape(1, 1, -1))\n",
    "    masks_down = cols < (y2.reshape(1, 1, -1))\n",
    "\n",
    "    crop_mask = masks_left * masks_right * masks_up * masks_down\n",
    "\n",
    "    return masks * crop_mask\n",
    "\n",
    "\n",
    "def mask_iou(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Inputs inputs are matricies of size _ x N. Output is size _1 x _2.\n",
    "    Note: if iscrowd is True, then mask2 should be the crowd.\n",
    "    \"\"\"\n",
    "    intersection = torch.matmul(mask1, mask2.t())\n",
    "    area1 = torch.sum(mask1, dim=1).reshape(1, -1)\n",
    "    area2 = torch.sum(mask2, dim=1).reshape(1, -1)\n",
    "    union = (area1.t() + area2) - intersection\n",
    "    ret = intersection / union\n",
    "\n",
    "    return ret.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pdb\n",
    "\n",
    "\n",
    "class PredictionModule(nn.Module):\n",
    "    def __init__(self, cfg, coef_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_classes = cfg.num_classes\n",
    "        self.coef_dim = coef_dim\n",
    "\n",
    "        self.upfeature = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.bbox_layer = nn.Conv2d(256, len(cfg.aspect_ratios) * 4, kernel_size=3, padding=1)\n",
    "        self.conf_layer = nn.Conv2d(256, len(cfg.aspect_ratios) * self.num_classes, kernel_size=3, padding=1)\n",
    "        self.coef_layer = nn.Sequential(nn.Conv2d(256, len(cfg.aspect_ratios) * self.coef_dim,\n",
    "                                                  kernel_size=3, padding=1),\n",
    "                                        nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upfeature(x)\n",
    "        conf = self.conf_layer(x).permute(0, 2, 3, 1).reshape(x.size(0), -1, self.num_classes)\n",
    "        box = self.bbox_layer(x).permute(0, 2, 3, 1).reshape(x.size(0), -1, 4)\n",
    "        coef = self.coef_layer(x).permute(0, 2, 3, 1).reshape(x.size(0), -1, self.coef_dim)\n",
    "        return conf, box, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, coef_dim):\n",
    "        super().__init__()\n",
    "        self.proto1 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.proto2 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(256, coef_dim, kernel_size=1, stride=1),\n",
    "                                    nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proto1(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.proto2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.lat_layers = nn.ModuleList([nn.Conv2d(x, 256, kernel_size=1) for x in self.in_channels])\n",
    "        self.pred_layers = nn.ModuleList([nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                                                        nn.ReLU(inplace=True)) for _ in self.in_channels])\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList([nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2),\n",
    "                                                              nn.ReLU(inplace=True)),\n",
    "                                                nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=2),\n",
    "                                                              nn.ReLU(inplace=True))])\n",
    "\n",
    "        self.upsample_module = nn.ModuleList([nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                                              nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)])\n",
    "\n",
    "    def forward(self, backbone_outs):\n",
    "        p5_1 = self.lat_layers[2](backbone_outs[2])\n",
    "        p5_upsample = self.upsample_module[1](p5_1)\n",
    "\n",
    "        p4_1 = self.lat_layers[1](backbone_outs[1]) + p5_upsample\n",
    "        p4_upsample = self.upsample_module[0](p4_1)\n",
    "\n",
    "        p3_1 = self.lat_layers[0](backbone_outs[0]) + p4_upsample\n",
    "\n",
    "        p5 = self.pred_layers[2](p5_1)\n",
    "        p4 = self.pred_layers[1](p4_1)\n",
    "        p3 = self.pred_layers[0](p3_1)\n",
    "\n",
    "        p6 = self.downsample_layers[0](p5)\n",
    "        p7 = self.downsample_layers[1](p6)\n",
    "\n",
    "        return p3, p4, p5, p6, p7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../weights/swin_tiny.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 518\u001b[0m\n\u001b[1;32m    515\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    517\u001b[0m net \u001b[38;5;241m=\u001b[39m SwinTransformer()\n\u001b[0;32m--> 518\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../weights/swin_tiny.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m aa \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m544\u001b[39m, \u001b[38;5;241m544\u001b[39m)\n\u001b[1;32m    520\u001b[0m out \u001b[38;5;241m=\u001b[39m net(aa)\n",
      "Cell \u001b[0;32mIn[18], line 489\u001b[0m, in \u001b[0;36mSwinTransformer.init_backbone\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    486\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mconstant_(m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(_init_weights)\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBackbone is initiated with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../weights/swin_tiny.pth'"
     ]
    }
   ],
   "source": [
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()  # binarize\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, window_size=(self.window_size, self.window_size), num_heads=num_heads)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = nn.LayerNorm(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, depth, num_heads, window_size=7, mlp_ratio=4., drop_path=0., downsample=None):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim,\n",
    "                                 num_heads=num_heads,\n",
    "                                 window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96):\n",
    "        super().__init__()\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pretrain_img_size (int): Input image size for training the pretrained model,\n",
    "            used in absolute postion embedding. Default 224.\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "        num_heads (tuple[int]): Number of attention head of each stage.\n",
    "        window_size (int): Window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrain_img_size=224, patch_size=4, in_chans=3, embed_dim=96, depths=(2, 2, 6, 2),\n",
    "                 num_heads=(3, 6, 12, 24), window_size=7, mlp_ratio=4., drop_path_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.out_norm_indices = (1, 2, 3)\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=mlp_ratio,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # add a norm layer for each output 1, 2, 3, output 0 is not used as a feature map\n",
    "        for i_layer in self.out_norm_indices:\n",
    "            self.add_module(f'norm{i_layer}', nn.LayerNorm(num_features[i_layer]))\n",
    "\n",
    "    def init_backbone(self, weight):\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "        self.apply(_init_weights)\n",
    "        self.load_state_dict(torch.load(weight), strict=False)\n",
    "        print(f'\\nBackbone is initiated with {weight}.\\n')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "\n",
    "            if i in self.out_norm_indices:\n",
    "                norm_layer = getattr(self, f'norm{i}')\n",
    "                x_out = norm_layer(x_out)\n",
    "\n",
    "            out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "            outs.append(out)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(10)\n",
    "    np.random.seed(10)\n",
    "\n",
    "    net = SwinTransformer()\n",
    "    net.init_backbone('../weights/swin_tiny.pth')\n",
    "    aa = torch.randn(1, 3, 544, 544)\n",
    "    out = net(aa)\n",
    "\n",
    "    for one in out:\n",
    "        print(one.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolact(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.coef_dim = 32\n",
    "\n",
    "        if cfg.__class__.__name__.startswith('res101'):\n",
    "            self.backbone = ResNet(layers=(3, 4, 23, 3))\n",
    "            self.fpn = FPN(in_channels=(512, 1024, 2048))\n",
    "        elif cfg.__class__.__name__.startswith('res50'):\n",
    "            self.backbone = ResNet(layers=(3, 4, 6, 3))\n",
    "            self.fpn = FPN(in_channels=(512, 1024, 2048))\n",
    "        elif cfg.__class__.__name__.startswith('swin_tiny'):\n",
    "            self.backbone = SwinTransformer()\n",
    "            self.fpn = FPN(in_channels=(192, 384, 768))\n",
    "\n",
    "        self.proto_net = ProtoNet(coef_dim=self.coef_dim)\n",
    "        self.prediction_layers = PredictionModule(cfg, coef_dim=self.coef_dim)\n",
    "\n",
    "        self.anchors = []\n",
    "        fpn_fm_shape = [math.ceil(cfg.img_size / stride) for stride in (8, 16, 32, 64, 128)]\n",
    "        for i, size in enumerate(fpn_fm_shape):\n",
    "            self.anchors += make_anchors(self.cfg, size, size, self.cfg.scales[i])\n",
    "\n",
    "        if cfg.mode == 'train':\n",
    "            self.semantic_seg_conv = nn.Conv2d(256, cfg.num_classes - 1, kernel_size=1)\n",
    "\n",
    "        # init weights, backbone weights will be covered later\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(module.weight.data)\n",
    "\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "\n",
    "    def load_weights(self, weight, cuda):\n",
    "        if cuda:\n",
    "            state_dict = torch.load(weight)\n",
    "        else:\n",
    "            state_dict = torch.load(weight, map_location='cpu')\n",
    "\n",
    "        for key in list(state_dict.keys()):\n",
    "            if self.cfg.mode != 'train' and key.startswith('semantic_seg_conv'):\n",
    "                del state_dict[key]\n",
    "\n",
    "        self.load_state_dict(state_dict, strict=True)\n",
    "        print(f'Model loaded with {weight}.\\n')\n",
    "        print(f'Number of all parameters: {sum([p.numel() for p in self.parameters()])}\\n')\n",
    "\n",
    "    def forward(self, img, box_classes=None, masks_gt=None):\n",
    "        outs = self.backbone(img)\n",
    "        outs = self.fpn(outs[1:4])\n",
    "        proto_out = self.proto_net(outs[0])  # feature map P3\n",
    "        proto_out = proto_out.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        class_pred, box_pred, coef_pred = [], [], []\n",
    "\n",
    "        for aa in outs:\n",
    "            class_p, box_p, coef_p = self.prediction_layers(aa)\n",
    "            class_pred.append(class_p)\n",
    "            box_pred.append(box_p)\n",
    "            coef_pred.append(coef_p)\n",
    "\n",
    "        class_pred = torch.cat(class_pred, dim=1)\n",
    "        box_pred = torch.cat(box_pred, dim=1)\n",
    "        coef_pred = torch.cat(coef_pred, dim=1)\n",
    "\n",
    "        if self.training:\n",
    "            seg_pred = self.semantic_seg_conv(outs[0])\n",
    "            return self.compute_loss(class_pred, box_pred, coef_pred, proto_out, seg_pred, box_classes, masks_gt)\n",
    "        else:\n",
    "            class_pred = F.softmax(class_pred, -1)\n",
    "            return class_pred, box_pred, coef_pred, proto_out\n",
    "\n",
    "    def compute_loss(self, class_p, box_p, coef_p, proto_p, seg_p, box_class, mask_gt):\n",
    "        device = class_p.device\n",
    "        class_gt = [None] * len(box_class)\n",
    "        batch_size = box_p.size(0)\n",
    "\n",
    "        if isinstance(self.anchors, list):\n",
    "            self.anchors = torch.tensor(self.anchors, device=device).reshape(-1, 4)\n",
    "\n",
    "        num_anchors = self.anchors.shape[0]\n",
    "\n",
    "        all_offsets = torch.zeros((batch_size, num_anchors, 4), dtype=torch.float32, device=device)\n",
    "        conf_gt = torch.zeros((batch_size, num_anchors), dtype=torch.int64, device=device)\n",
    "        anchor_max_gt = torch.zeros((batch_size, num_anchors, 4), dtype=torch.float32, device=device)\n",
    "        anchor_max_i = torch.zeros((batch_size, num_anchors), dtype=torch.int64, device=device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            box_gt = box_class[i][:, :-1]\n",
    "            class_gt[i] = box_class[i][:, -1].long()\n",
    "\n",
    "            all_offsets[i], conf_gt[i], anchor_max_gt[i], anchor_max_i[i] = match(self.cfg, box_gt,\n",
    "                                                                                  self.anchors, class_gt[i])\n",
    "\n",
    "        # all_offsets: the transformed box coordinate offsets of each pair of anchor and gt box\n",
    "        # conf_gt: the foreground and background labels according to the 'pos_thre' and 'neg_thre',\n",
    "        #          '0' means background, '>0' means foreground.\n",
    "        # anchor_max_gt: the corresponding max IoU gt box for each anchor\n",
    "        # anchor_max_i: the index of the corresponding max IoU gt box for each anchor\n",
    "        assert (not all_offsets.requires_grad) and (not conf_gt.requires_grad) and \\\n",
    "               (not anchor_max_i.requires_grad), 'Incorrect computation graph, check the grad.'\n",
    "\n",
    "        # only compute losses from positive samples\n",
    "        pos_bool = conf_gt > 0\n",
    "\n",
    "        loss_c = self.category_loss(class_p, conf_gt, pos_bool)\n",
    "        loss_b = self.box_loss(box_p, all_offsets, pos_bool)\n",
    "        loss_m = self.lincomb_mask_loss(pos_bool, anchor_max_i, coef_p, proto_p, mask_gt, anchor_max_gt)\n",
    "        loss_s = self.semantic_seg_loss(seg_p, mask_gt, class_gt)\n",
    "        return loss_c, loss_b, loss_m, loss_s\n",
    "\n",
    "    def category_loss(self, class_p, conf_gt, pos_bool, np_ratio=3):\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = class_p.reshape(-1, self.cfg.num_classes)\n",
    "\n",
    "        batch_conf_max = batch_conf.max()\n",
    "        mark = torch.log(torch.sum(torch.exp(batch_conf - batch_conf_max), 1)) + batch_conf_max - batch_conf[:, 0]\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        mark = mark.reshape(class_p.size(0), -1)\n",
    "        mark[pos_bool] = 0  # filter out pos boxes\n",
    "        mark[conf_gt < 0] = 0  # filter out neutrals (conf_gt = -1)\n",
    "\n",
    "        _, idx = mark.sort(1, descending=True)\n",
    "        _, idx_rank = idx.sort(1)\n",
    "\n",
    "        num_pos = pos_bool.long().sum(1, keepdim=True)\n",
    "        num_neg = torch.clamp(np_ratio * num_pos, max=pos_bool.size(1) - 1)\n",
    "        neg_bool = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Just in case there aren't enough negatives, don't start using positives as negatives\n",
    "        neg_bool[pos_bool] = 0\n",
    "        neg_bool[conf_gt < 0] = 0  # Filter out neutrals\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        class_p_mined = class_p[(pos_bool + neg_bool)].reshape(-1, self.cfg.num_classes)\n",
    "        class_gt_mined = conf_gt[(pos_bool + neg_bool)]\n",
    "\n",
    "        return self.cfg.conf_alpha * F.cross_entropy(class_p_mined, class_gt_mined, reduction='sum') / num_pos.sum()\n",
    "\n",
    "    def box_loss(self, box_p, all_offsets, pos_bool):\n",
    "        num_pos = pos_bool.sum()\n",
    "        pos_box_p = box_p[pos_bool, :]\n",
    "        pos_offsets = all_offsets[pos_bool, :]\n",
    "\n",
    "        return self.cfg.bbox_alpha * F.smooth_l1_loss(pos_box_p, pos_offsets, reduction='sum') / num_pos\n",
    "\n",
    "    def lincomb_mask_loss(self, pos_bool, anchor_max_i, coef_p, proto_p, mask_gt, anchor_max_gt):\n",
    "        proto_h, proto_w = proto_p.shape[1:3]\n",
    "        total_pos_num = pos_bool.sum()\n",
    "        loss_m = 0\n",
    "        for i in range(coef_p.size(0)):\n",
    "            # downsample the gt mask to the size of 'proto_p'\n",
    "            downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (proto_h, proto_w), mode='bilinear',\n",
    "                                              align_corners=False).squeeze(0)\n",
    "            downsampled_masks = downsampled_masks.permute(1, 2, 0).contiguous()\n",
    "            # binarize the gt mask because of the downsample operation\n",
    "            downsampled_masks = downsampled_masks.gt(0.5).float()\n",
    "\n",
    "            pos_anchor_i = anchor_max_i[i][pos_bool[i]]\n",
    "            pos_anchor_box = anchor_max_gt[i][pos_bool[i]]\n",
    "            pos_coef = coef_p[i][pos_bool[i]]\n",
    "\n",
    "            if pos_anchor_i.size(0) == 0:\n",
    "                continue\n",
    "\n",
    "            # If exceeds the number of masks for training, select a random subset\n",
    "            old_num_pos = pos_coef.size(0)\n",
    "            if old_num_pos > self.cfg.masks_to_train:\n",
    "                perm = torch.randperm(pos_coef.size(0))\n",
    "                select = perm[:self.cfg.masks_to_train]\n",
    "                pos_coef = pos_coef[select]\n",
    "                pos_anchor_i = pos_anchor_i[select]\n",
    "                pos_anchor_box = pos_anchor_box[select]\n",
    "\n",
    "            num_pos = pos_coef.size(0)\n",
    "\n",
    "            pos_mask_gt = downsampled_masks[:, :, pos_anchor_i]\n",
    "\n",
    "            # mask assembly by linear combination\n",
    "            # @ means dot product\n",
    "            mask_p = torch.sigmoid(proto_p[i] @ pos_coef.t())\n",
    "            mask_p = crop(mask_p, pos_anchor_box)  # pos_anchor_box.shape: (num_pos, 4)\n",
    "            # TODO: grad out of gt box is 0, should it be modified?\n",
    "            # TODO: need an upsample before computing loss?\n",
    "            mask_loss = F.binary_cross_entropy(torch.clamp(mask_p, 0, 1), pos_mask_gt, reduction='none')\n",
    "            # mask_loss = -pos_mask_gt*torch.log(mask_p) - (1-pos_mask_gt) * torch.log(1-mask_p)\n",
    "\n",
    "            # Normalize the mask loss to emulate roi pooling's effect on loss.\n",
    "            anchor_area = (pos_anchor_box[:, 2] - pos_anchor_box[:, 0]) * (pos_anchor_box[:, 3] - pos_anchor_box[:, 1])\n",
    "            mask_loss = mask_loss.sum(dim=(0, 1)) / anchor_area\n",
    "\n",
    "            if old_num_pos > num_pos:\n",
    "                mask_loss *= old_num_pos / num_pos\n",
    "\n",
    "            loss_m += torch.sum(mask_loss)\n",
    "\n",
    "        return self.cfg.mask_alpha * loss_m / proto_h / proto_w / total_pos_num\n",
    "\n",
    "    def semantic_seg_loss(self, segmentation_p, mask_gt, class_gt):\n",
    "        # Note classes here exclude the background class, so num_classes = cfg.num_classes - 1\n",
    "        batch_size, num_classes, mask_h, mask_w = segmentation_p.size()\n",
    "        loss_s = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            cur_segment = segmentation_p[i]\n",
    "            cur_class_gt = class_gt[i]\n",
    "\n",
    "            downsampled_masks = F.interpolate(mask_gt[i].unsqueeze(0), (mask_h, mask_w), mode='bilinear',\n",
    "                                              align_corners=False).squeeze(0)\n",
    "            downsampled_masks = downsampled_masks.gt(0.5).float()\n",
    "\n",
    "            # Construct Semantic Segmentation\n",
    "            segment_gt = torch.zeros_like(cur_segment, requires_grad=False)\n",
    "            for j in range(downsampled_masks.size(0)):\n",
    "                segment_gt[cur_class_gt[j]] = torch.max(segment_gt[cur_class_gt[j]], downsampled_masks[j])\n",
    "\n",
    "            loss_s += F.binary_cross_entropy_with_logits(cur_segment, segment_gt, reduction='sum')\n",
    "\n",
    "        return self.cfg.semantic_alpha * loss_s / mask_h / mask_w / batch_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
