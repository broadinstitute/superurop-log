{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireModule(nn.Module):\n",
    "    def __init__(self, in_c: int, squeeze_c: int, out_c: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        # for all layers below, stride is default = 1\n",
    "\n",
    "        # first point-wise convolution: 1x1\n",
    "        self.squeeze = nn.Conv2d(in_channels=in_c, out_channels=squeeze_c, kernel_size=1, stride=stride)\n",
    "        # first independent conv: 3x3\n",
    "        self.expand1x1 = nn.Conv2d(in_channels=squeeze_c, out_channels=out_c // 2, kernel_size=3, padding=1)\n",
    "        # second independent (pointwise) conv: 1x1\n",
    "        self.expand3x3 = nn.Conv2d(in_channels=squeeze_c, out_channels=out_c // 2, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.squeeze(x))\n",
    "        x = torch.cat([\n",
    "            self.relu(self.expand1x1(x)),\n",
    "            self.relu(self.expand3x3(x)),\n",
    "        ], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "\n",
    "    def __init__(self, in_c: int, squeeze_c: int, out_c: int):\n",
    "        super().__init__()\n",
    "        self.fire_mods = nn.Sequential(\n",
    "            FireModule(in_c=in_c, squeeze_c=squeeze_c, out_c=out_c // 2, stride=2),\n",
    "            FireModule(in_c=out_c // 2, squeeze_c=squeeze_c * 2, out_c=out_c, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fire_mods(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposedFireModule(nn.Module):\n",
    "    def __init__(self, in_c: int, squeeze_c: int, out_c: int, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.t_squeeze_1x1 = nn.ConvTranspose2d(in_channels=in_c, out_channels=squeeze_c, kernel_size=1)\n",
    "        self.t_expand_1x1 = nn.ConvTranspose2d(in_channels=squeeze_c, out_channels=out_c // 2, kernel_size=1)\n",
    "        self.t_expand_2x2 = nn.ConvTranspose2d(in_channels=out_c * 2, out_channels=out_c // 2, kernel_size=2, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.t_squeeze_1x1(x))  # squeeze\n",
    "        # inception stage\n",
    "        x = torch.cat([\n",
    "            self.relu(self.t_expand_1x1(x)),\n",
    "            self.relu(self.t_expand_2x2(x)),\n",
    "        ], dim=1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_c: int, t_fire_mod_out_c: int, out_c: int):\n",
    "        super().__init__()\n",
    "        self.t_fire_mod = TransposedFireModule(in_c=in_c, squeeze_c=in_c // 2, out_c=t_fire_mod_out_c)\n",
    "        self.fire_mods = nn.Sequential(\n",
    "            FireModule(in_c=t_fire_mod_out_c * 2,  # twice since transpose fire mod output is concatenated\n",
    "                       squeeze_c=t_fire_mod_out_c, # squeeze with half the input size\n",
    "                       out_c=out_c * 2,\n",
    "                       stride=1),\n",
    "            FireModule(in_c=out_c * 2, squeeze_c=out_c, out_c=out_c, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x1: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.t_fire_mod(x)\n",
    "        x = torch.cat([x, x1,], dim=1)\n",
    "        x = self.fire_mods(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeUnet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # first two 3x3 conv layers (3, 3, 64)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        # contracting step\n",
    "        self.ds1 = DownSample(in_c=64, squeeze_c=32, out_c=256)\n",
    "        self.ds2 = DownSample(in_c=256, squeeze_c=48, out_c=1_024)\n",
    "        self.ds3 = DownSample(in_c=1_024, squeeze_c=64, out_c=4_096)\n",
    "        self.ds4 = DownSample(in_c=4_096, squeeze_c=80, out_c=16_384)\n",
    "\n",
    "        # expanding step\n",
    "        self.us1 = UpSample(in_c=16_384, t_fire_mod_out_c=4_096, out_c=2_048)\n",
    "        self.us2 = UpSample(in_c=2_048, t_fire_mod_out_c=1_024, out_c=512)\n",
    "        self.us3 = UpSample(in_c=512, t_fire_mod_out_c=256, out_c=128)\n",
    "\n",
    "        self.t_conv = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.outc = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # pre steps\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x_conv1 = self.relu(self.conv2(x)) # skip connection to last concat after upsample\n",
    "\n",
    "        # 4 downsampling steps\n",
    "        x_ds1 = self.ds1(x=x_conv1)  # skip connection to UpSample(US) #3\n",
    "        x_ds2 = self.ds2(x=x_ds1)  # skip connection to US #2\n",
    "        x_ds3 = self.ds3(x=x_ds2)  # skip connection to US #1\n",
    "        x_ds4 = self.ds4(x=x_ds3)\n",
    "\n",
    "        # 3 upsampling steps\n",
    "        x = self.us1(x_ds4, x_ds3)\n",
    "        x = self.us2(x, x_ds2)\n",
    "        x = self.us3(x, x_ds1)\n",
    "\n",
    "        # post steps\n",
    "        x = self.relu(self.t_conv(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "\n",
    "        logits = nn.ReLU(self.outc(x))\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m512\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sq_unet \u001b[39m=\u001b[39m SqueezeUnet(num_classes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sq_unet(t)\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# first = nn.Sequential(\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#     nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# x_ds3 = self.down_sample(x=x_ds2, in_c=256, squeeze_c=64, expand_c=256)  # skip connection to US #1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# x_ds4 = self.down_sample(x=x_ds3, in_c=512, squeeze_c=80, expand_c=512)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m x_ds4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds4(x\u001b[39m=\u001b[39mx_ds3)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# 3 upsampling steps\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mus1(x_ds4, x_ds3)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mus2(x, x_ds2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mus3(x, x_ds1)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor, x1: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_fire_mod(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, x1,], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfire_mods(x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_squeeze_1x1(x))  \u001b[39m# squeeze\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# inception stage\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_expand_1x1(x)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_expand_2x2(x)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m ], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ogola/superurop/superurop-log/experiments/squeeze-unet/squeeze-unet.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "t = torch.rand(1, 3, 512, 512)\n",
    "sq_unet = SqueezeUnet(num_classes=2)\n",
    "sq_unet(t).shape\n",
    "\n",
    "# first = nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "#     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "# )\n",
    "# first_out = first(t)\n",
    "# print(first_out.shape)\n",
    "# fire = FireModule(in_c=64, squeeze_c=32, expand_c=64)\n",
    "# fire2 = FireModule(in_c=128, squeeze_c=64, expand_c=128)\n",
    "# fire_out = fire(first_out)\n",
    "# print(fire_out.shape)\n",
    "# fire2_out = fire2(fire_out)\n",
    "# fire2_out.shape\n",
    "# x_ds1 = self.down_sample(x=x_conv1, in_c=64, squeeze_c=32, expand_c=64)  # skip connection to UpSample(US) #3\n",
    "# x_ds2 = self.down_sample(x=x_ds1, in_c=128, squeeze_c=48, expand_c=128)  # skip connection to US #2\n",
    "# x_ds3 = self.down_sample(x=x_ds2, in_c=256, squeeze_c=64, expand_c=256)  # skip connection to US #1\n",
    "# x_ds4 = self.down_sample(x=x_ds3, in_c=512, squeeze_c=80, expand_c=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 3 up sampling modules\\n\\n# US 1\\nt_fire_mod1 = TransposedFireModule(in_c=16_384, squeeze_c=8192, out_c=4096)\\n# concatenation step\\n# torch.cat((t_fire_mod1(x_ds4), x_ds3), dim=1)\\n# fire modules\\nmods = nn.Sequential(\\n            FireModule(in_c=8192, squeeze_c=4096, out_c=4096, stride=1),\\n            FireModule(in_c=4096, squeeze_c=4096 // 2, out_c=2048, stride=1),\\n)\\n\\n# US 2\\nt_fire_mod2 = TransposedFireModule(in_c=2048, squeeze_c=1024, out_c=1024)\\n# concatenation step\\n# torch.cat((t_fire_mod2(x_ds4), x_ds3), dim=1)\\n# fire modules\\nmods = nn.Sequential(\\n            FireModule(in_c=2048, squeeze_c=1024, out_c=1024, stride=1),\\n            FireModule(in_c=1024, squeeze_c=1024 // 2, out_c=512, stride=1),\\n)\\n\\n# US 3\\nt_fire_mod3 = TransposedFireModule(in_c=512, squeeze_c=256, out_c=256)\\n# concatenation step\\n# torch.cat((t_fire_mod1(x_ds4), x_ds3), dim=1)\\n# fire modules\\nmods = nn.Sequential(\\n            FireModule(in_c=512, squeeze_c=256, out_c=256, stride=1),\\n            FireModule(in_c=256, squeeze_c=256 // 2, out_c=128, stride=1),\\n)\\n\\n# transposed conv layer\\nt_conv = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=1)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 3 up sampling modules\n",
    "\n",
    "# US 1\n",
    "t_fire_mod1 = TransposedFireModule(in_c=16_384, squeeze_c=8192, out_c=4096)\n",
    "# concatenation step\n",
    "# torch.cat((t_fire_mod1(x_ds4), x_ds3), dim=1)\n",
    "# fire modules\n",
    "mods = nn.Sequential(\n",
    "            FireModule(in_c=8192, squeeze_c=4096, out_c=4096, stride=1),\n",
    "            FireModule(in_c=4096, squeeze_c=4096 // 2, out_c=2048, stride=1),\n",
    ")\n",
    "\n",
    "# US 2\n",
    "t_fire_mod2 = TransposedFireModule(in_c=2048, squeeze_c=1024, out_c=1024)\n",
    "# concatenation step\n",
    "# torch.cat((t_fire_mod2(x_ds4), x_ds3), dim=1)\n",
    "# fire modules\n",
    "mods = nn.Sequential(\n",
    "            FireModule(in_c=2048, squeeze_c=1024, out_c=1024, stride=1),\n",
    "            FireModule(in_c=1024, squeeze_c=1024 // 2, out_c=512, stride=1),\n",
    ")\n",
    "\n",
    "# US 3\n",
    "t_fire_mod3 = TransposedFireModule(in_c=512, squeeze_c=256, out_c=256)\n",
    "# concatenation step\n",
    "# torch.cat((t_fire_mod1(x_ds4), x_ds3), dim=1)\n",
    "# fire modules\n",
    "mods = nn.Sequential(\n",
    "            FireModule(in_c=512, squeeze_c=256, out_c=256, stride=1),\n",
    "            FireModule(in_c=256, squeeze_c=256 // 2, out_c=128, stride=1),\n",
    ")\n",
    "\n",
    "# transposed conv layer\n",
    "t_conv = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
